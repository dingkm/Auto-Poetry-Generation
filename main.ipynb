{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf8\n",
    "import sys, os\n",
    "import torch as t\n",
    "from data import get_data\n",
    "from model import PoetryModel\n",
    "from torch import nn\n",
    "from utils import Visualizer\n",
    "import tqdm\n",
    "from torchnet import meter\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    data_path = 'data/'  # 诗歌的文本文件存放路径\n",
    "    pickle_path = 'data/tang.npz'  # 预处理好的二进制文件\n",
    "    author = None  # 只学习某位作者的诗歌\n",
    "    constrain = None  # 长度限制\n",
    "    category = 'poet.tang'  # 类别，唐诗还是宋诗歌(poet.song)\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-4\n",
    "    use_gpu = True\n",
    "    epoch = 20\n",
    "    batch_size = 128\n",
    "    maxlen = 125  # 超过这个长度的之后字被丢弃，小于这个长度的在前面补空格\n",
    "    plot_every = 20  # 每20个batch 可视化一次\n",
    "    # use_env = True # 是否使用visodm\n",
    "    env = 'poetry'  # visdom env\n",
    "    max_gen_len = 200  # 生成诗歌最长长度\n",
    "    debug_file = '/tmp/debugp'\n",
    "    model_path = None  # 预训练模型路径\n",
    "    prefix_words = '细雨鱼儿出,微风燕子斜。'  # 不是诗歌的组成部分，用来控制生成诗歌的意境\n",
    "    start_words = '闲云潭影日悠悠'  # 诗歌开始\n",
    "    acrostic = False  # 是否是藏头诗\n",
    "    model_prefix = 'checkpoints/tang'  # 模型保存路径\n",
    "\n",
    "opt = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, start_words, ix2word, word2ix, prefix_words=None):\n",
    "    \"\"\"\n",
    "    给定几个词，根据这几个词接着生成一首完整的诗歌\n",
    "    start_words：u'春江潮水连海平'\n",
    "    比如start_words 为 春江潮水连海平，可以生成：\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    results = list(start_words)\n",
    "    start_word_len = len(start_words)\n",
    "    # 手动设置第一个词为<START>\n",
    "    input = t.Tensor([word2ix['<START>']]).view(1, 1).long()\n",
    "    if opt.use_gpu: input = input.cuda()\n",
    "    hidden = None\n",
    "\n",
    "    if prefix_words:\n",
    "        for word in prefix_words:\n",
    "            output, hidden = model(input, hidden)\n",
    "            input = input.data.new([word2ix[word]]).view(1, 1)\n",
    "\n",
    "    for i in range(opt.max_gen_len):\n",
    "        output, hidden = model(input, hidden)\n",
    "\n",
    "        if i < start_word_len:\n",
    "            w = results[i]\n",
    "            input = input.data.new([word2ix[w]]).view(1, 1)\n",
    "        else:\n",
    "            top_index = output.data[0].topk(1)[1][0].item()\n",
    "            w = ix2word[top_index]\n",
    "            results.append(w)\n",
    "            input = input.data.new([top_index]).view(1, 1)\n",
    "        if w == '<EOP>':\n",
    "            del results[-1]\n",
    "            break\n",
    "    return results\n",
    "\n",
    "\n",
    "def gen_acrostic(model, start_words, ix2word, word2ix, prefix_words=None):\n",
    "    \"\"\"\n",
    "    生成藏头诗\n",
    "    start_words : u'深度学习'\n",
    "    生成：\n",
    "    深木通中岳，青苔半日脂。\n",
    "    度山分地险，逆浪到南巴。\n",
    "    学道兵犹毒，当时燕不移。\n",
    "    习根通古岸，开镜出清羸。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    start_word_len = len(start_words)\n",
    "    input = (t.Tensor([word2ix['<START>']]).view(1, 1).long())\n",
    "    if opt.use_gpu: input = input.cuda()\n",
    "    hidden = None\n",
    "\n",
    "    index = 0  # 用来指示已经生成了多少句藏头诗\n",
    "    # 上一个词\n",
    "    pre_word = '<START>'\n",
    "\n",
    "    if prefix_words:\n",
    "        for word in prefix_words:\n",
    "            output, hidden = model(input, hidden)\n",
    "            input = (input.data.new([word2ix[word]])).view(1, 1)\n",
    "\n",
    "    for i in range(opt.max_gen_len):\n",
    "        output, hidden = model(input, hidden)\n",
    "        top_index = output.data[0].topk(1)[1][0].item()\n",
    "        w = ix2word[top_index]\n",
    "\n",
    "        if (pre_word in {u'。', u'！', '<START>'}):\n",
    "            # 如果遇到句号，藏头的词送进去生成\n",
    "\n",
    "            if index == start_word_len:\n",
    "                # 如果生成的诗歌已经包含全部藏头的词，则结束\n",
    "                break\n",
    "            else:\n",
    "                # 把藏头的词作为输入送入模型\n",
    "                w = start_words[index]\n",
    "                index += 1\n",
    "                input = (input.data.new([word2ix[w]])).view(1, 1)\n",
    "        else:\n",
    "            # 否则的话，把上一次预测是词作为下一个词输入\n",
    "            input = (input.data.new([word2ix[w]])).view(1, 1)\n",
    "        results.append(w)\n",
    "        pre_word = w\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(**kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        setattr(opt, k, v)\n",
    "\n",
    "    opt.device=t.device('cuda') if opt.use_gpu else t.device('cpu')\n",
    "    device = opt.device\n",
    "    vis = Visualizer(env=opt.env)\n",
    "\n",
    "    # 获取数据\n",
    "    data, word2ix, ix2word = get_data(opt)\n",
    "    data = t.from_numpy(data)\n",
    "    dataloader = t.utils.data.DataLoader(data,\n",
    "                                         batch_size=opt.batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=1)\n",
    "\n",
    "    # 模型定义\n",
    "    model = PoetryModel(len(word2ix), 128, 256)\n",
    "    print('the length in train ',len(word2ix))\n",
    "    optimizer = t.optim.Adam(model.parameters(), lr=opt.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if opt.model_path:\n",
    "        model.load_state_dict(t.load(opt.model_path))\n",
    "    model.to(device)\n",
    "\n",
    "    loss_meter = meter.AverageValueMeter()\n",
    "    for epoch in range(opt.epoch):\n",
    "        loss_meter.reset()\n",
    "        for ii, data_ in tqdm.tqdm(enumerate(dataloader)):\n",
    "\n",
    "            # 训练\n",
    "            data_ = data_.long().transpose(1, 0).contiguous()\n",
    "            data_ = data_.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            input_, target = data_[:-1, :], data_[1:, :]\n",
    "            output, _ = model(input_)\n",
    "            loss = criterion(output, target.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_meter.add(loss.item())\n",
    "\n",
    "            # 可视化\n",
    "            if (1 + ii) % opt.plot_every == 0:\n",
    "\n",
    "                if os.path.exists(opt.debug_file):\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "                vis.plot('loss', loss_meter.value()[0])\n",
    "\n",
    "                # 诗歌原文\n",
    "                poetrys = [[ix2word[_word] for _word in data_[:, _iii].tolist()]\n",
    "                           for _iii in range(data_.shape[1])][:16]\n",
    "                vis.text('</br>'.join([''.join(poetry) for poetry in poetrys]), win=u'origin_poem')\n",
    "\n",
    "                gen_poetries = []\n",
    "                # 分别以这几个字作为诗歌的第一个字，生成8首诗\n",
    "                for word in list(u'春江花月夜凉如水'):\n",
    "                    gen_poetry = ''.join(generate(model, word, ix2word, word2ix))\n",
    "                    gen_poetries.append(gen_poetry)\n",
    "                vis.text('</br>'.join([''.join(poetry) for poetry in gen_poetries]), win=u'gen_poem')\n",
    "\n",
    "        t.save(model.state_dict(), '%s_%s.pth' % (opt.model_prefix, epoch))\n",
    "\n",
    "\n",
    "def gen(**kwargs):\n",
    "    \"\"\"\n",
    "    提供命令行接口，用以生成相应的诗\n",
    "    \"\"\"\n",
    "\n",
    "    for k, v in kwargs.items():\n",
    "        setattr(opt, k, v)\n",
    "        \n",
    "        \n",
    "        \n",
    "    data, word2ix, ix2word = get_data(opt)\n",
    "#     data = t.from_numpy(data)\n",
    "#     dataloader = t.utils.data.DataLoader(data,\n",
    "#                                          batch_size=opt.batch_size,\n",
    "#                                          shuffle=True,\n",
    "#                                          num_workers=1)\n",
    "\n",
    "    # 模型定义\n",
    "    model = PoetryModel(len(word2ix), 128, 256)\n",
    "    print('the length in test ',len(word2ix))\n",
    "    \n",
    "    \n",
    "    data, word2ix, ix2word = get_data(opt)\n",
    "    model = PoetryModel(len(word2ix), 128, 256);\n",
    "    print('the length ',len(word2ix))\n",
    "    print(PoetryModel)\n",
    "    map_location = lambda s, l: s\n",
    "    \n",
    "    #model = nn.DataParallel(model)\n",
    "    \n",
    "    state_dict = t.load(opt.model_path, map_location=map_location)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    if opt.use_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    # python2和python3 字符串兼容\n",
    "    if sys.version_info.major == 3:\n",
    "        if opt.start_words.isprintable():\n",
    "            start_words = opt.start_words\n",
    "            prefix_words = opt.prefix_words if opt.prefix_words else None\n",
    "        else:\n",
    "            start_words = opt.start_words.encode('ascii', 'surrogateescape').decode('utf8')\n",
    "            prefix_words = opt.prefix_words.encode('ascii', 'surrogateescape').decode(\n",
    "                'utf8') if opt.prefix_words else None\n",
    "    else:\n",
    "        start_words = opt.start_words.decode('utf8')\n",
    "        prefix_words = opt.prefix_words.decode('utf8') if opt.prefix_words else None\n",
    "\n",
    "    start_words = start_words.replace(',', u'，') \\\n",
    "        .replace('.', u'。') \\\n",
    "        .replace('?', u'？')\n",
    "\n",
    "    gen_poetry = gen_acrostic if opt.acrostic else generate\n",
    "    result = gen_poetry(model, start_words, ix2word, word2ix, prefix_words)\n",
    "    print(''.join(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Without the incoming socket you cannot receive events from the server or register event handlers to your Visdom client.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length in train  8293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450it [01:27,  5.16it/s]\n",
      "450it [01:27,  5.15it/s]\n",
      "450it [01:28,  5.11it/s]\n",
      "450it [01:28,  5.09it/s]\n",
      "450it [01:29,  5.01it/s]\n",
      "450it [01:28,  5.11it/s]\n",
      "450it [01:28,  5.08it/s]\n",
      "450it [01:28,  5.10it/s]\n",
      "450it [01:29,  5.04it/s]\n",
      "450it [01:28,  5.07it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     import fire\n",
    "\n",
    "#     fire.Fire()\n",
    "    parameters = {'plot_every':200,'batch_size':128,'pickle_path':'data/tang.npz','env':'poetry','epoch':10,\n",
    "                  }\n",
    "    \n",
    "    train(**parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "embeddings.weight \t torch.Size([8293, 128])\n",
      "lstm.weight_ih_l0 \t torch.Size([1024, 128])\n",
      "lstm.weight_hh_l0 \t torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0 \t torch.Size([1024])\n",
      "lstm.bias_hh_l0 \t torch.Size([1024])\n",
      "lstm.weight_ih_l1 \t torch.Size([1024, 256])\n",
      "lstm.weight_hh_l1 \t torch.Size([1024, 256])\n",
      "lstm.bias_ih_l1 \t torch.Size([1024])\n",
      "lstm.bias_hh_l1 \t torch.Size([1024])\n",
      "linear1.weight \t torch.Size([8293, 256])\n",
      "linear1.bias \t torch.Size([8293])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "model = PoetryModel(8293,128,256)\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "    \n",
    "state_dict = t.load('checkpoints/tang_1.pth')\n",
    "model.load_state_dict(state_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57580, 125)\n",
      "[8292 8292 8292 8292 8292 8292 8292 8292 8292 8292 8292 8292 8292 8292\n",
      " 8292 8292 8292 8292 8292 8292 8292 8292 8292 8292 8292 8292 8292 8292\n",
      " 8292 8292 8292 8292 8292 8292 8292 8292 8292 8292 8292 8291 2309 2596\n",
      " 6483 2260 7316 7066 6332 5274 2125 5029 7792 7435 4186 8087 7047 6622\n",
      " 6933 7066 6134 3564 3766 6920 6157 7435 7086 4770 5849 4776 4981 7066\n",
      " 4857 2649 3020  332 1727 7435 7458 7294 3465 5149 1671 7066 2834 6000\n",
      " 3942 3534 1534 7435 4102 7460  758 3961 3374 7066 7904 6811 4449 2121\n",
      " 6802 7435 6182   27 7912 1756 7440 7066  201 7909 8118  201 4662 7435\n",
      " 7824 1508 3154  152 5862 7066 7976 6043  258   47 7878 7435 8290]\n",
      "耀\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "data, word2ix, ix2word = get_data(opt)\n",
    "print(data.shape)\n",
    "print(data[1])\n",
    "print(ix2word[1])\n",
    "print(word2ix['耀'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoetryModel(\n",
      "  (embeddings): Embedding(3, 128)\n",
      "  (lstm): LSTM(128, 256, num_layers=2)\n",
      "  (linear1): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n",
      "the length in test  8293\n",
      "the length  8293\n",
      "<class 'model.PoetryModel'>\n",
      "吾亦生死別，何由问长流。父子已及朝，翩翩準自周。小壺劝我酒，左右皆具修。丁来视箧笥，为客唯布裘。\n"
     ]
    }
   ],
   "source": [
    "parameters = {'model_path':'checkpoints/tang_199.pth','pickle_path':'data/tang.npz','start_words':'深度学习',\n",
    "              'prefix_words':'江流天地外，山色有无中。','acrostic':True}\n",
    "\n",
    "para2 = {'model_path':'checkpoints/tang_199.pth','pickle_path':'data/tang.npz','start_words':'吾父小丁',\n",
    "         'prefix_words':'亲朋无一字，老病有孤舟。'}\n",
    "#gen(**parameters)\n",
    "\n",
    "#gen(**para2)\n",
    "from torchsummary import summary\n",
    "print(PoetryModel(3,128,256))\n",
    "gen(**para2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
